{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom keras.applications import ResNet50\nfrom keras.applications.resnet50 import preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import SGD, Adam\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\nfrom keras.layers import Flatten, Dense, Dropout, BatchNormalization, UpSampling2D\nfrom keras.utils import to_categorical\nfrom keras.datasets import cifar10\nimport pandas as pd\nimport pickle \n\ndef resnet_accuracies(froze=False, LR_scheduler='Plateau', augmentation=True, batch_size=100, epochs=20, learn_rate=.001):\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    x_train = preprocess_input(x_train)\n    x_test = preprocess_input(x_test)\n    \n    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3)\n\n    y_train = to_categorical(y_train)\n    y_val = to_categorical(y_val)\n    y_test = to_categorical(y_test)\n\n    train_generator = ImageDataGenerator(\n        rotation_range=2,\n        horizontal_flip=True,\n        zoom_range=.1)\n\n    val_generator = ImageDataGenerator(\n        rotation_range=2,\n        horizontal_flip=True,\n        zoom_range=.1)\n\n    test_generator = ImageDataGenerator(\n        rotation_range=2,\n        horizontal_flip=True,\n        zoom_range=.1)\n    train_generator.fit(x_train)\n    val_generator.fit(x_val)\n    test_generator.fit(x_test) \n    \n    # setting learning rate schedulers\n    def scheduler(epoch):\n        if epoch < 5:\n            return learn_rate \n        if epoch < 20:\n            return learn_rate /10\n        return learn_rate/100\n    if LR_scheduler == 'Custom' :\n        callbacks = [LearningRateScheduler(scheduler)]\n    elif LR_scheduler == 'Plateau' :\n        callbacks = [ReduceLROnPlateau(\n        monitor='val_accuracy',\n        factor=.01,\n        patience=3,\n        min_lr=1e-5)]\n    else :\n        callbacks = [LearningRateScheduler(lambda x: learn_rate)]\n    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3),\n                          classes=y_train.shape[1])\n\n    model = Sequential()\n    model.add(UpSampling2D(size=(2,2)))\n    model.add(UpSampling2D(size=(2,2))) \n    model.add(base_model)\n    model.add(Flatten())\n    model.add(BatchNormalization(momentum=0.9, epsilon=1e-5))\n    model.add(Dense(256, activation='relu', input_dim=512))\n    model.add(Dropout(.5))\n    model.add(BatchNormalization(momentum=0.9, epsilon=1e-5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(.5))\n    model.add(BatchNormalization(momentum=0.9, epsilon=1e-5))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(.5))\n    model.add(BatchNormalization(momentum=0.9, epsilon=1e-5))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(10, activation='softmax'))\n\n    model.layers[3].trainable = not froze\n\n    adam = Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n    if augmentation:\n        history = model.fit(train_generator.flow(x_train, y_train, batch_size=batch_size),\n                  epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n                  validation_data=val_generator.flow(x_val, y_val, batch_size=batch_size), validation_steps=10,\n                  callbacks=callbacks, verbose=1)\n    else:\n        history = model.fit(x_train, y_train, batch_size=batch_size,\n                  epochs=epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n                  validation_data=(x_val, y_val), validation_steps=10,\n                  callbacks=callbacks, verbose=1)\n    return {**history.history, 'test_accuracy': (model.predict(x_test).argmax(axis=1) == y_test.argmax(axis=1)).mean()}\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"experiments = [{'name': 'Base model',\n                'kwargs': {}},\n               {'name': 'Frozen',\n                'kwargs': {'froze': True}},\n               {'name': 'No scheduler',\n                'kwargs': {'LR_scheduler': None}},\n               {'name': 'Custom scheduler',\n                'kwargs': {'LR_scheduler': 'Custom'}},\n               {'name': 'No augmentation',\n                'kwargs': {'augmentation': False}},\n               \n               \n              ]\nfor i in range(3):\n    assessment = pd.DataFrame([{**resnet_accuracies(**exp['kwargs']),\n                               'name': exp['name']} for exp in experiments])\n    assessment.to_pickle(f'assessment_{i}.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n350/350 [==============================] - 73s 190ms/step - loss: 2.0517 - accuracy: 0.2510 - val_loss: 2.0485 - val_accuracy: 0.3700\nEpoch 2/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.1979 - accuracy: 0.5801 - val_loss: 1.6253 - val_accuracy: 0.5040\nEpoch 3/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.9516 - accuracy: 0.6844 - val_loss: 2.9719 - val_accuracy: 0.4060\nEpoch 4/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.8022 - accuracy: 0.7468 - val_loss: 1.0105 - val_accuracy: 0.6950\nEpoch 5/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.7204 - accuracy: 0.7751 - val_loss: 0.7145 - val_accuracy: 0.7720\nEpoch 6/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.6830 - accuracy: 0.7926 - val_loss: 0.8271 - val_accuracy: 0.7570\nEpoch 7/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.6023 - accuracy: 0.8170 - val_loss: 0.6008 - val_accuracy: 0.8090\nEpoch 8/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.5518 - accuracy: 0.8357 - val_loss: 0.5612 - val_accuracy: 0.8190\nEpoch 9/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.5159 - accuracy: 0.8456 - val_loss: 0.6503 - val_accuracy: 0.8080\nEpoch 10/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.4951 - accuracy: 0.8531 - val_loss: 0.7616 - val_accuracy: 0.7620\nEpoch 11/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.4564 - accuracy: 0.8649 - val_loss: 0.5832 - val_accuracy: 0.8190\nEpoch 12/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.4354 - accuracy: 0.8713 - val_loss: 0.4008 - val_accuracy: 0.8900\nEpoch 13/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.3966 - accuracy: 0.8859 - val_loss: 0.3816 - val_accuracy: 0.8900\nEpoch 14/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.3744 - accuracy: 0.8896 - val_loss: 0.4188 - val_accuracy: 0.8730\nEpoch 15/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.3525 - accuracy: 0.8987 - val_loss: 0.3207 - val_accuracy: 0.8980\nEpoch 16/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.3418 - accuracy: 0.9015 - val_loss: 0.4010 - val_accuracy: 0.8730\nEpoch 17/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.3282 - accuracy: 0.9067 - val_loss: 0.3264 - val_accuracy: 0.9070\nEpoch 18/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.3171 - accuracy: 0.9084 - val_loss: 0.3410 - val_accuracy: 0.8960\nEpoch 19/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.3137 - accuracy: 0.9095 - val_loss: 0.3354 - val_accuracy: 0.9010\nEpoch 20/20\n350/350 [==============================] - 66s 187ms/step - loss: 0.2976 - accuracy: 0.9141 - val_loss: 0.3626 - val_accuracy: 0.8750\nEpoch 1/20\n350/350 [==============================] - 73s 191ms/step - loss: 2.1502 - accuracy: 0.2095 - val_loss: 1.5034 - val_accuracy: 0.4270\nEpoch 2/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.3475 - accuracy: 0.5083 - val_loss: 2.1119 - val_accuracy: 0.4020\nEpoch 3/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.0766 - accuracy: 0.6289 - val_loss: 0.9192 - val_accuracy: 0.6530\nEpoch 4/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.8907 - accuracy: 0.7069 - val_loss: 1.0970 - val_accuracy: 0.6730\nEpoch 5/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.7869 - accuracy: 0.7488 - val_loss: 0.7919 - val_accuracy: 0.7540\nEpoch 6/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.7021 - accuracy: 0.7864 - val_loss: 0.8281 - val_accuracy: 0.7420\nEpoch 7/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.6404 - accuracy: 0.8057 - val_loss: 0.7729 - val_accuracy: 0.7650\nEpoch 8/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.6044 - accuracy: 0.8173 - val_loss: 0.5921 - val_accuracy: 0.8150\nEpoch 9/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.5504 - accuracy: 0.8363 - val_loss: 0.6741 - val_accuracy: 0.7720\nEpoch 10/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.5211 - accuracy: 0.8468 - val_loss: 0.7778 - val_accuracy: 0.7780\nEpoch 11/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.4778 - accuracy: 0.8556 - val_loss: 0.5767 - val_accuracy: 0.8210\nEpoch 12/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.4595 - accuracy: 0.8649 - val_loss: 0.7275 - val_accuracy: 0.7900\nEpoch 13/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.4466 - accuracy: 0.8687 - val_loss: 0.8662 - val_accuracy: 0.7570\nEpoch 14/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.4064 - accuracy: 0.8812 - val_loss: 0.5816 - val_accuracy: 0.8280\nEpoch 15/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.3925 - accuracy: 0.8853 - val_loss: 0.5456 - val_accuracy: 0.8240\nEpoch 16/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.3620 - accuracy: 0.8930 - val_loss: 0.4730 - val_accuracy: 0.8670\nEpoch 17/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.3552 - accuracy: 0.8968 - val_loss: 0.5520 - val_accuracy: 0.8440\nEpoch 18/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.3384 - accuracy: 0.9003 - val_loss: 0.6250 - val_accuracy: 0.8190\nEpoch 19/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.3431 - accuracy: 0.9008 - val_loss: 0.3907 - val_accuracy: 0.8830\nEpoch 20/20\n350/350 [==============================] - 66s 188ms/step - loss: 0.3005 - accuracy: 0.9136 - val_loss: 0.4722 - val_accuracy: 0.8560\nEpoch 1/20\n350/350 [==============================] - 73s 191ms/step - loss: 2.4328 - accuracy: 0.1139 - val_loss: 2.4632 - val_accuracy: 0.2110\nEpoch 2/20\n350/350 [==============================] - 66s 188ms/step - loss: 2.0795 - accuracy: 0.1973 - val_loss: 2.1931 - val_accuracy: 0.1850\nEpoch 3/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.8116 - accuracy: 0.2821 - val_loss: 12.0245 - val_accuracy: 0.2940\nEpoch 4/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.7707 - accuracy: 0.3009 - val_loss: 2.9678 - val_accuracy: 0.3220\nEpoch 5/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.5569 - accuracy: 0.3806 - val_loss: 1.6720 - val_accuracy: 0.3430\nEpoch 6/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.4524 - accuracy: 0.4252 - val_loss: 2.0555 - val_accuracy: 0.3620\nEpoch 7/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.3501 - accuracy: 0.4728 - val_loss: 1.3216 - val_accuracy: 0.4970\nEpoch 8/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.2886 - accuracy: 0.5090 - val_loss: 1.9939 - val_accuracy: 0.3710\nEpoch 9/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.2375 - accuracy: 0.5513 - val_loss: 1.8947 - val_accuracy: 0.5960\nEpoch 10/20\n350/350 [==============================] - 66s 188ms/step - loss: 1.0750 - accuracy: 0.6233 - val_loss: 1.2576 - val_accuracy: 0.5580\nEpoch 11/20\n278/350 [======================>.......] - ETA: 13s - loss: 1.0828 - accuracy: 0.6192","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}